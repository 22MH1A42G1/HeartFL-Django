{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717e2745",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Model - Training & Testing\n",
    "## Federated Learning Heart Disease Prediction System\n",
    "\n",
    "This notebook covers:\n",
    "1. Data Loading and Exploration\n",
    "2. Data Preprocessing\n",
    "3. Model Training (Multiple Algorithms)\n",
    "4. Model Evaluation\n",
    "5. Model Comparison\n",
    "6. Model Saving for Production Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bdbd5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab5935",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f242ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'media/datasets/MH-HOSP-2024-001_1769405831_Hospital_A.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Total Records: {len(df)}\")\n",
    "print(f\"Total Features: {len(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de7657",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "if missing_df['Missing Values'].sum() == 0:\n",
    "    print(\"\\n‚úì No missing values found!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Total missing values: {missing_df['Missing Values'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION (HeartDisease)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_counts = df['HeartDisease'].value_counts()\n",
    "target_percentage = df['HeartDisease'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nClass 0 (No Disease): {target_counts[0]} ({target_percentage[0]:.2f}%)\")\n",
    "print(f\"Class 1 (Disease): {target_counts[1]} ({target_percentage[1]:.2f}%)\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='HeartDisease', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Heart Disease Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Heart Disease (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%',\n",
    "            startangle=90, colors=['#90EE90', '#FF6B6B'])\n",
    "axes[1].set_title('Heart Disease Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = target_counts[1] / target_counts[0]\n",
    "if 0.8 <= imbalance_ratio <= 1.2:\n",
    "    print(\"\\n‚úì Dataset is balanced!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Dataset may be imbalanced (ratio: {imbalance_ratio:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(df[feature].value_counts())\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eaf4d6",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features distribution\n",
    "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.histplot(data=df, x=feature, hue='HeartDisease', kde=True, ax=axes[i], palette='Set1')\n",
    "    axes[i].set_title(f'{feature} Distribution by Heart Disease', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(feature, fontsize=11)\n",
    "    axes[i].set_ylabel('Count', fontsize=11)\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    pd.crosstab(df[feature], df['HeartDisease']).plot(kind='bar', ax=axes[i], color=['#90EE90', '#FF6B6B'])\n",
    "    axes[i].set_title(f'{feature} vs Heart Disease', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(feature, fontsize=11)\n",
    "    axes[i].set_ylabel('Count', fontsize=11)\n",
    "    axes[i].legend(['No Disease', 'Disease'])\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea86263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (after encoding)\n",
    "# Create a copy for correlation analysis\n",
    "df_corr = df.copy()\n",
    "\n",
    "# Encode categorical variables for correlation\n",
    "le = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    df_corr[col] = le.fit_transform(df_corr[col])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df_corr.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with target\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURES CORRELATION WITH TARGET (HeartDisease)\")\n",
    "print(\"=\" * 80)\n",
    "target_corr = correlation_matrix['HeartDisease'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec48048",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Encode categorical variables\n",
    "print(\"\\n1. Encoding categorical variables...\")\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"   ‚úì {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 2. Handle missing values (if any)\n",
    "print(\"\\n2. Handling missing values...\")\n",
    "if df_processed.isnull().sum().sum() > 0:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed = pd.DataFrame(imputer.fit_transform(df_processed), columns=df_processed.columns)\n",
    "    print(\"   ‚úì Missing values imputed\")\n",
    "else:\n",
    "    print(\"   ‚úì No missing values found\")\n",
    "\n",
    "# 3. Remove outliers in cholesterol (0 values are likely missing)\n",
    "print(\"\\n3. Handling outliers...\")\n",
    "cholesterol_zero_count = (df_processed['Cholesterol'] == 0).sum()\n",
    "if cholesterol_zero_count > 0:\n",
    "    print(f\"   ‚ö† Found {cholesterol_zero_count} zero cholesterol values\")\n",
    "    # Replace 0 with median\n",
    "    median_chol = df_processed[df_processed['Cholesterol'] > 0]['Cholesterol'].median()\n",
    "    df_processed.loc[df_processed['Cholesterol'] == 0, 'Cholesterol'] = median_chol\n",
    "    print(f\"   ‚úì Replaced with median value: {median_chol}\")\n",
    "else:\n",
    "    print(\"   ‚úì No significant outliers in cholesterol\")\n",
    "\n",
    "# Similarly for RestingBP\n",
    "bp_zero_count = (df_processed['RestingBP'] == 0).sum()\n",
    "if bp_zero_count > 0:\n",
    "    print(f\"   ‚ö† Found {bp_zero_count} zero blood pressure values\")\n",
    "    median_bp = df_processed[df_processed['RestingBP'] > 0]['RestingBP'].median()\n",
    "    df_processed.loc[df_processed['RestingBP'] == 0, 'RestingBP'] = median_bp\n",
    "    print(f\"   ‚úì Replaced with median value: {median_bp}\")\n",
    "else:\n",
    "    print(\"   ‚úì No significant outliers in blood pressure\")\n",
    "\n",
    "print(\"\\n‚úì Data preprocessing completed!\")\n",
    "print(f\"\\nProcessed dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c63f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('HeartDisease', axis=1)\n",
    "y = df_processed['HeartDisease']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURES AND TARGET SEPARATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} ({(X_train.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} ({(X_test.shape[0]/len(X))*100:.1f}%)\")\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTarget distribution in testing set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42931705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SCALING (Standardization)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úì Features scaled using StandardScaler\")\n",
    "print(\"\\nScaled training data statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"Std: {X_train_scaled.std(axis=0).round(4)}\")\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8edd9",
   "metadata": {},
   "source": [
    "## 6. Model Training - Multiple Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining {len(models)} different models...\\n\")\n",
    "\n",
    "# Dictionary to store trained models and results\n",
    "trained_models = {}\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'MCC': mcc,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std\n",
    "    })\n",
    "    \n",
    "    # Store trained model\n",
    "    trained_models[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Accuracy: {accuracy:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f if roc_auc else 'N/A'}\\n\")\n",
    "\n",
    "print(\"\\n‚úì All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792d67f",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Accuracy']\n",
    "print(f\"\\n\\nüèÜ BEST MODEL: {best_model_name} (Accuracy: {best_accuracy:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "results_df.plot(x='Model', y='Accuracy', kind='bar', ax=axes[0, 0], color='skyblue', legend=False)\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_xlabel('')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_ylim([0.5, 1.0])\n",
    "axes[0, 0].axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Precision, Recall, F1 Comparison\n",
    "results_df.plot(x='Model', y=['Precision', 'Recall', 'F1-Score'], kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Precision, Recall, F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score', fontsize=12)\n",
    "axes[0, 1].set_xlabel('')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_ylim([0.5, 1.0])\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. ROC-AUC Comparison\n",
    "roc_data = results_df[results_df['ROC-AUC'].notna()]\n",
    "roc_data.plot(x='Model', y='ROC-AUC', kind='bar', ax=axes[1, 0], color='coral', legend=False)\n",
    "axes[1, 0].set_title('ROC-AUC Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[1, 0].set_xlabel('')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# 4. Cross-Validation Scores\n",
    "results_df.plot(x='Model', y='CV Mean', kind='bar', ax=axes[1, 1], \n",
    "                color='lightgreen', legend=False, yerr=results_df['CV Std'])\n",
    "axes[1, 1].set_title('Cross-Validation Mean Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('CV Accuracy', fontsize=12)\n",
    "axes[1, 1].set_xlabel('')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_ylim([0.5, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838996f",
   "metadata": {},
   "source": [
    "## 8. Detailed Evaluation - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73285752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model details\n",
    "best_model_info = trained_models[best_model_name]\n",
    "best_model = best_model_info['model']\n",
    "y_pred_best = best_model_info['y_pred']\n",
    "y_pred_proba_best = best_model_info['y_pred_proba']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"DETAILED EVALUATION - {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives: {cm[0][0]}\")\n",
    "print(f\"False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]}\")\n",
    "print(f\"True Positives: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287549ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Normalized Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "axes[1].set_title(f'Normalized Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for best model\n",
    "if y_pred_proba_best is not None:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n\", feature_importance.to_string(index=False))\n",
    "    \n",
    "    # Visualize Feature Importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\n‚ö† {best_model_name} does not support feature importance visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1ac8a",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aec6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"HYPERPARAMETER TUNING - {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPerforming Grid Search for optimal hyperparameters...\\n\")\n",
    "\n",
    "# Define parameter grid based on best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }\n",
    "elif best_model_name == 'SVM':\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "else:\n",
    "    print(f\"‚ö† No predefined hyperparameter grid for {best_model_name}\")\n",
    "    print(\"Using default parameters...\")\n",
    "    param_grid = {}\n",
    "\n",
    "if param_grid:\n",
    "    # Perform Grid Search\n",
    "    grid_search = GridSearchCV(\n",
    "        best_model, param_grid, cv=5, scoring='accuracy', \n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GRID SEARCH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use tuned model\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nTuned Model Test Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"Original Model Test Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"Improvement: {(accuracy_tuned - best_accuracy):.4f}\")\n",
    "    \n",
    "    # Update best model if tuned is better\n",
    "    if accuracy_tuned > best_accuracy:\n",
    "        best_model = tuned_model\n",
    "        print(\"\\n‚úì Using tuned model for final deployment\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Original model performs better, keeping original\")\n",
    "else:\n",
    "    print(\"\\n‚úì Using trained model without hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac085d",
   "metadata": {},
   "source": [
    "## 10. Model Saving for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757204f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ml_models directory if it doesn't exist\n",
    "model_dir = 'ml_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING MODELS FOR PRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(model_dir, 'heart_disease_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\n‚úì Best model saved: {model_path}\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\n‚úì Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save label encoders\n",
    "encoders_path = os.path.join(model_dir, 'label_encoders.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"\\n‚úì Label encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = os.path.join(model_dir, 'feature_names.pkl')\n",
    "with open(feature_names_path, 'wb') as f:\n",
    "    pickle.dump(list(X.columns), f)\n",
    "print(f\"\\n‚úì Feature names saved: {feature_names_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': best_accuracy,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': list(X.columns),\n",
    "    'target_classes': ['No Disease', 'Disease']\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, 'model_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"\\n‚úì Model metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì ALL MODELS AND ARTIFACTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThese files can now be used in the Django application for predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1558f",
   "metadata": {},
   "source": [
    "## 11. Model Testing - Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf10cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test with a few samples\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(X_test.index, n_samples, replace=False)\n",
    "\n",
    "print(f\"\\nMaking predictions on {n_samples} random test samples...\\n\")\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    # Get sample\n",
    "    sample = X_test.loc[idx].values.reshape(1, -1)\n",
    "    sample_scaled = scaler.transform(sample)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(sample_scaled)[0]\n",
    "    probability = best_model.predict_proba(sample_scaled)[0] if hasattr(best_model, 'predict_proba') else None\n",
    "    \n",
    "    # Get actual value\n",
    "    actual = y_test.loc[idx]\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"   Features: {dict(zip(X.columns, X_test.loc[idx].values))}\")\n",
    "    print(f\"   Actual: {'Disease' if actual == 1 else 'No Disease'}\")\n",
    "    print(f\"   Predicted: {'Disease' if prediction == 1 else 'No Disease'}\")\n",
    "    if probability is not None:\n",
    "        print(f\"   Probability: {probability[1]*100:.2f}% (Disease)\")\n",
    "    print(f\"   Result: {'‚úì CORRECT' if prediction == actual else '‚úó INCORRECT'}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe44a4",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68453f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAnalysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nDataset: {data_path}\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Total Features: {len(X.columns)}\")\n",
    "print(f\"\\nTraining Set Size: {len(X_train)} samples\")\n",
    "print(f\"Testing Set Size: {len(X_test)} samples\")\n",
    "print(f\"\\nModels Trained: {len(models)}\")\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "if results_df.iloc[0]['ROC-AUC']:\n",
    "    print(f\"   ‚Ä¢ ROC-AUC: {results_df.iloc[0]['ROC-AUC']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MCC: {results_df.iloc[0]['MCC']:.4f}\")\n",
    "print(f\"\\nModel Artifacts Saved:\")\n",
    "print(f\"   ‚úì {model_path}\")\n",
    "print(f\"   ‚úì {scaler_path}\")\n",
    "print(f\"   ‚úì {encoders_path}\")\n",
    "print(f\"   ‚úì {feature_names_path}\")\n",
    "print(f\"   ‚úì {metadata_path}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì MODEL TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Review the model performance metrics above\")\n",
    "print(\"2. If satisfied, the model files are ready for production use\")\n",
    "print(\"3. Integrate the saved model with the Django application\")\n",
    "print(\"4. Test predictions through the web interface\")\n",
    "print(\"5. Monitor model performance in production\")\n",
    "print(\"\\nNote: Model files are saved in the 'ml_models' directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
